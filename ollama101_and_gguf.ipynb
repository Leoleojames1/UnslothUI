{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama 101 - A set of basic ollama functions from the ollama pypi docs\n",
    "All functions are referenced from the PyPi ollama package built by Jeffrey Morgan: https://pypi.org/project/ollama/\n",
    "\n",
    "The following notebook is a set of ollama tools and part of an ollama guide made by Leo Borcherding. \n",
    "This notebook is part of the following ollama tutorial https://github.com/Leoleojames1/ollamaStarterKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama pull/run\n",
    "\n",
    "The following commands will allow you to pull any ollama model, feel free to explore ollama's vast set of models to choose from, here are some of my custom system prompts: https://ollama.com/borch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama run phi3\n",
    "\n",
    "&\n",
    "\n",
    "ollama pull phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama show & list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ollama show, shows the currently loaded model's modelfile metadata*\n",
    "ollama show --modelfile llama3.2:3b\n",
    "\n",
    "*ollama list shows the available models*\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Ollama library\n",
    "import ollama\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Show information about a specific model\n",
    "model_info = ollama.show('llama3.2:3b')\n",
    "\n",
    "# Convert to a dictionary for easier formatting\n",
    "model_dict = {\n",
    "    \"model\": \"llama3.2:3b\",\n",
    "    \"modified_at\": str(model_info.modified_at),\n",
    "    \"family\": model_info.details.family,\n",
    "    \"parameter_size\": model_info.details.parameter_size,\n",
    "    \"quantization\": model_info.details.quantization_level,\n",
    "    \"parameters\": model_info.parameters.split('\\n')\n",
    "}\n",
    "\n",
    "# Pretty print the model information\n",
    "print(json.dumps(model_dict, indent=2))\n",
    "\n",
    "# List all available models\n",
    "models = ollama.list()\n",
    "\n",
    "# Print a simplified table of models\n",
    "print(\"\\n{:<40} {:<10} {:<10} {:<10}\".format(\"MODEL\", \"FAMILY\", \"SIZE\", \"QUANT\"))\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for model in models.models:\n",
    "    size_gb = f\"{model.size / 1_000_000_000:.2f} GB\"\n",
    "    print(\"{:<40} {:<10} {:<10} {:<10}\".format(\n",
    "        model.model[:38], \n",
    "        model.details.family[:8], \n",
    "        size_gb, \n",
    "        model.details.quantization_level[:8]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ChatResponse method can be used for direct back and forth messaging with your ollama model, but will not provide access to prompt streaming. For Streaming follow along to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2:3b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Who are you? And what were you trained on?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a nice interactive chat interface for your Ollama models that works directly in the notebook. This interface includes:\n",
    "\n",
    "1. A dropdown to select the ollama model\n",
    "2. A text input for messages\n",
    "3. A scrollable chat history display\n",
    "4. A reset button to start fresh conversations\n",
    "5. Streaming responses that appear character by character\n",
    "\n",
    "To use it, simply run this cell and you'll get a fully functional chat interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class SimpleOllamaChat:\n",
    "    def __init__(self, model=\"llama3.2:3b\"):\n",
    "        \"\"\"Initialize the chat interface with a specified model.\"\"\"\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.running = True\n",
    "        print(f\"ü§ñ Chat initialized with model: {self.model}\")\n",
    "        print(\"Type 'exit' to end the chat, 'clear' to reset the conversation, or 'change model:[name]' to switch models.\")\n",
    "        \n",
    "    def display_models(self):\n",
    "        \"\"\"Display available models in a clean format.\"\"\"\n",
    "        try:\n",
    "            models_list = ollama.list()\n",
    "            print(\"\\nAvailable models:\")\n",
    "            print(\"-\" * 60)\n",
    "            for model in models_list.models[:10]:  # Show only first 10 models to avoid clutter\n",
    "                print(f\"‚Ä¢ {model.model}\")\n",
    "            if len(models_list.models) > 10:\n",
    "                print(f\"... and {len(models_list.models) - 10} more models\")\n",
    "            print(\"-\" * 60)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error fetching models: {e}\")\n",
    "    \n",
    "    def start_chat(self):\n",
    "        \"\"\"Start the interactive chat loop.\"\"\"\n",
    "        while self.running:\n",
    "            # Get user input\n",
    "            user_message = input(\"\\nüë§ You: \")\n",
    "            \n",
    "            # Process commands\n",
    "            if user_message.lower() == 'exit':\n",
    "                print(\"üëã Ending chat session.\")\n",
    "                self.running = False\n",
    "                break\n",
    "                \n",
    "            elif user_message.lower() == 'clear':\n",
    "                self.messages = []\n",
    "                clear_output(wait=True)\n",
    "                print(f\"ü§ñ Chat reset with model: {self.model}\")\n",
    "                print(\"Type 'exit' to end the chat, 'clear' to reset the conversation, or 'change model:[name]' to switch models.\")\n",
    "                continue\n",
    "                \n",
    "            elif user_message.lower().startswith('change model:'):\n",
    "                new_model = user_message[len('change model:'):].strip()\n",
    "                if new_model:\n",
    "                    self.model = new_model\n",
    "                    print(f\"üîÑ Model changed to {self.model}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    self.display_models()\n",
    "                    continue\n",
    "                    \n",
    "            elif user_message.lower() == 'models':\n",
    "                self.display_models()\n",
    "                continue\n",
    "                \n",
    "            # Skip empty messages\n",
    "            if not user_message.strip():\n",
    "                continue\n",
    "                \n",
    "            # Add user message to history\n",
    "            self.messages.append({'role': 'user', 'content': user_message})\n",
    "            \n",
    "            # Get AI response with streaming\n",
    "            print(\"\\nü§ñ AI: \", end='')\n",
    "            \n",
    "            try:\n",
    "                full_response = \"\"\n",
    "                stream = ollama.chat(\n",
    "                    model=self.model,\n",
    "                    messages=self.messages,\n",
    "                    stream=True,\n",
    "                )\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    if 'message' in chunk and 'content' in chunk['message']:\n",
    "                        text_chunk = chunk['message']['content']\n",
    "                        full_response += text_chunk\n",
    "                        print(text_chunk, end='', flush=True)\n",
    "                \n",
    "                # Add AI response to history\n",
    "                self.messages.append({'role': 'assistant', 'content': full_response})\n",
    "                print()  # Add newline after response\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è Error: {str(e)}\")\n",
    "                \n",
    "                # If model not found, suggest listing models\n",
    "                if \"model not found\" in str(e).lower():\n",
    "                    print(f\"Model '{self.model}' not found. Type 'models' to see available models.\")\n",
    "                    \n",
    "                    # Reset to a likely available model\n",
    "                    default_models = [\"llama3.2:3b\", \"llama3.2\", \"llama3\", \"phi3\"]\n",
    "                    for model in default_models:\n",
    "                        try:\n",
    "                            # Try to pull model info to check if it exists\n",
    "                            ollama.show(model)\n",
    "                            self.model = model\n",
    "                            print(f\"Switched to model: {self.model}\")\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "# Create and start the chat interface\n",
    "chat = SimpleOllamaChat()\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Modelfile create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create a coding assistant from an existing model\\ncoding_system_prompt = \"\"\"You are CodingLlama, an expert programming assistant.\\nFocus on providing clean, efficient, and well-documented code examples.\\nAlways include explanations of how the code works.\\nIf you\\'re unsure about any part of your solution, acknowledge it and suggest alternatives.\"\"\"\\n\\ncreate_and_deploy_custom_model(\\n    model_name=\"CodingLlama\",\\n    base_model=\"llama3.2:3b\",\\n    system_prompt=coding_system_prompt,\\n    temperature=0.2,  # Lower temperature for more precise coding answers\\n    context_length=4096,\\n    test_prompt=\"Write a Python function to count word frequencies in a text file\"\\n)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from typing import Optional, Dict, Any, List, Union, Tuple\n",
    "import time\n",
    "import re\n",
    "\n",
    "def generate_modelfile(\n",
    "    model_name: str,\n",
    "    base_model: str = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "    system_prompt: str = \"\",\n",
    "    parameters: Dict[str, Any] = None,\n",
    "    output_dir: str = \"./modelfiles\",\n",
    "    filename: str = None,\n",
    "    tags: List[str] = None,\n",
    "    license: str = \"Apache-2.0\",\n",
    "    export_to_ollama: bool = False,\n",
    "    ollama_name: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a Modelfile for use with Ollama from Unsloth fine-tuned models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        A name for your custom model\n",
    "    base_model : str\n",
    "        Base model name, either local path or Hugging Face ID\n",
    "    system_prompt : str\n",
    "        Custom system prompt for the model\n",
    "    parameters : dict\n",
    "        Model parameters like temperature, top_p, etc.\n",
    "    output_dir : str\n",
    "        Directory to save the Modelfile\n",
    "    filename : str\n",
    "        Custom filename for the Modelfile (defaults to model_name.Modelfile)\n",
    "    tags : list\n",
    "        Tags to categorize the model\n",
    "    license : str\n",
    "        License type for the model\n",
    "    export_to_ollama : bool\n",
    "        Whether to create an Ollama model from this Modelfile\n",
    "    ollama_name : str\n",
    "        Custom name for the Ollama model (defaults to model_name)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the generated Modelfile\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set default parameters if none provided\n",
    "    if parameters is None:\n",
    "        parameters = {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"num_ctx\": 4096,\n",
    "            \"stop\": [\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        }\n",
    "    \n",
    "    # Set default filename if none provided\n",
    "    if filename is None:\n",
    "        # Replace any slashes with hyphens for the filename\n",
    "        safe_model_name = model_name.replace(\"/\", \"-\")\n",
    "        filename = f\"{safe_model_name}.Modelfile\"\n",
    "    \n",
    "    # Set default ollama_name if none provided\n",
    "    if ollama_name is None:\n",
    "        # Remove any slashes and spaces for Ollama model name\n",
    "        ollama_name = model_name.split(\"/\")[-1].lower().replace(\" \", \"-\")\n",
    "    \n",
    "    # Format the system prompt for the Modelfile\n",
    "    formatted_system_prompt = system_prompt.replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n",
    "    \n",
    "    # Build the Modelfile content\n",
    "    modelfile_content = f'FROM {base_model}\\n\\n'\n",
    "    \n",
    "    if system_prompt:\n",
    "        modelfile_content += f'SYSTEM \"\"\"{system_prompt}\"\"\"\\n\\n'\n",
    "    \n",
    "    # Add parameters\n",
    "    for param_name, param_value in parameters.items():\n",
    "        if isinstance(param_value, list):\n",
    "            # Handle lists like stop words\n",
    "            param_str = json.dumps(param_value)\n",
    "            modelfile_content += f'PARAMETER {param_name} {param_str}\\n'\n",
    "        else:\n",
    "            modelfile_content += f'PARAMETER {param_name} {param_value}\\n'\n",
    "    \n",
    "    # Add template if needed for specific models\n",
    "    if \"llama\" in base_model.lower() or \"llama3\" in base_model.lower():\n",
    "        modelfile_content += '\\n# Using Llama 3 template\\n'\n",
    "        modelfile_content += 'TEMPLATE \"\"\"{{- if .System }}<|start_header_id|>system<|end_header_id|>\\n\\n{{ .System }}<|eot_id|>\\n\\n{{- end }}{{ range $i, $message := .Messages }}{{- if eq $message.Role \"user\" }}<|start_header_id|>user<|end_header_id|>\\n\\n{{ $message.Content }}<|eot_id|>\\n\\n{{- else if eq $message.Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ $message.Content }}<|eot_id|>\\n\\n{{- end }}{{ end }}<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\\n'\n",
    "    elif \"phi-3\" in base_model.lower():\n",
    "        modelfile_content += '\\n# Using Phi-3 template\\n'\n",
    "        modelfile_content += 'TEMPLATE \"\"\"{{- if .System }}<|system|>\\n{{ .System }}\\n<|user|>\\n{{- else }}<|user|>\\n{{- end }}{{ range $i, $message := .Messages }}{{- if eq $message.Role \"user\" }}{{ $message.Content }}\\n<|assistant|>\\n{{- else if eq $message.Role \"assistant\" }}{{ $message.Content }}\\n<|user|>\\n{{- end }}{{ end }}\"\"\"\\n'\n",
    "    \n",
    "    # Add tags if provided\n",
    "    if tags:\n",
    "        tags_str = \", \".join(tags)\n",
    "        modelfile_content += f'\\nTAGS {tags_str}\\n'\n",
    "    \n",
    "    # Add license if provided\n",
    "    if license:\n",
    "        modelfile_content += f'LICENSE {license}\\n'\n",
    "    \n",
    "    # Save the Modelfile\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(modelfile_content)\n",
    "    \n",
    "    print(f\"‚úÖ Modelfile generated at: {output_path}\")\n",
    "    \n",
    "    # Create Ollama model if requested\n",
    "    if export_to_ollama:\n",
    "        try:\n",
    "            print(f\"üîÑ Creating Ollama model '{ollama_name}'...\")\n",
    "            # Check if Ollama is installed\n",
    "            import subprocess\n",
    "            result = subprocess.run([\"ollama\", \"create\", ollama_name, \"-f\", output_path], \n",
    "                                 capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ Ollama model '{ollama_name}' created successfully!\")\n",
    "                print(f\"   You can now use it with: ollama run {ollama_name}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to create Ollama model: {result.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating Ollama model: {str(e)}\")\n",
    "            print(\"   Make sure Ollama is installed and running.\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def extract_modelfile(model_name: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Extracts the Modelfile from an existing Ollama model using 'ollama show'.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the existing Ollama model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[bool, str]\n",
    "        Success status and either the Modelfile content or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run 'ollama show --modelfile model_name' command\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"show\", \"--modelfile\", model_name],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            return True, result.stdout\n",
    "        else:\n",
    "            return False, f\"Error: {result.stderr}\"\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return False, f\"Failed to extract Modelfile: {e.stderr}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error: {str(e)}\"\n",
    "\n",
    "def list_ollama_models() -> List[str]:\n",
    "    \"\"\"\n",
    "    Get a list of all available Ollama models.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[str]\n",
    "        List of model names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run 'ollama list' command\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"list\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            # Parse the output to extract model names\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            # Skip the header line and extract the first column (model name)\n",
    "            models = []\n",
    "            for line in lines[1:]:  # Skip header\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    models.append(line.split()[0])  # First column is model name\n",
    "            return models\n",
    "        else:\n",
    "            print(f\"Error listing models: {result.stderr}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Function to create a custom Unsloth ModelFile and deploy it to Ollama\n",
    "def create_and_deploy_custom_model(\n",
    "    model_name: str,\n",
    "    base_model: str = \"llama3.2:3b\",\n",
    "    system_prompt: str = None,\n",
    "    temperature: float = 0.7,\n",
    "    context_length: int = 4096,\n",
    "    stop_tokens: List[str] = None,\n",
    "    test_prompt: str = \"Tell me what you can do\",\n",
    "    ollama_export: bool = True,\n",
    "    from_existing_model: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a custom ModelFile for use with Ollama and optionally deploys it.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name for your custom model\n",
    "    base_model : str\n",
    "        Base model to use (e.g., \"llama3.2:3b\", \"phi3:latest\")\n",
    "    system_prompt : str\n",
    "        Custom system prompt\n",
    "    temperature : float\n",
    "        Model temperature (0.0-2.0)\n",
    "    context_length : int\n",
    "        Context window size\n",
    "    stop_tokens : list\n",
    "        Custom stop tokens\n",
    "    test_prompt : str\n",
    "        Test prompt to use when demonstrating the model\n",
    "    ollama_export : bool\n",
    "        Whether to export to Ollama\n",
    "    from_existing_model : bool\n",
    "        If True, extract the Modelfile from the base_model rather than creating a new one\n",
    "    \"\"\"\n",
    "    # Generate timestamp for unique file naming\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = f\"./modelfiles_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if from_existing_model:\n",
    "        # Extract the existing Modelfile\n",
    "        success, modelfile_content = extract_modelfile(base_model)\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"‚ùå {modelfile_content}\")\n",
    "            return None\n",
    "            \n",
    "        # Modify the Modelfile as needed\n",
    "        # 1. Change the FROM line if needed\n",
    "        if base_model != model_name:\n",
    "            modelfile_content = re.sub(r'^FROM .*$', f'FROM {base_model}', modelfile_content, flags=re.MULTILINE)\n",
    "        \n",
    "        # 2. Update or add SYSTEM prompt if provided\n",
    "        if system_prompt is not None:\n",
    "            # Check if a SYSTEM prompt already exists\n",
    "            if 'SYSTEM' in modelfile_content:\n",
    "                # Replace existing SYSTEM prompt\n",
    "                modelfile_content = re.sub(\n",
    "                    r'SYSTEM\\s+[\"\"\"].*?[\"\"\"]', \n",
    "                    f'SYSTEM \"\"\"{system_prompt}\"\"\"', \n",
    "                    modelfile_content, \n",
    "                    flags=re.DOTALL\n",
    "                )\n",
    "            else:\n",
    "                # Add SYSTEM prompt after FROM line\n",
    "                modelfile_content = re.sub(\n",
    "                    r'(FROM .*?)(\\n\\n|\\n)', \n",
    "                    f'\\\\1\\\\2SYSTEM \"\"\"{system_prompt}\"\"\"\\\\2', \n",
    "                    modelfile_content\n",
    "                )\n",
    "        \n",
    "        # 3. Update temperature parameter if specified\n",
    "        if 'PARAMETER temperature' in modelfile_content:\n",
    "            modelfile_content = re.sub(\n",
    "                r'PARAMETER temperature .*', \n",
    "                f'PARAMETER temperature {temperature}', \n",
    "                modelfile_content\n",
    "            )\n",
    "        else:\n",
    "            # Add temperature parameter if it doesn't exist\n",
    "            modelfile_content += f\"\\nPARAMETER temperature {temperature}\\n\"\n",
    "        \n",
    "        # 4. Update context length if specified\n",
    "        if 'PARAMETER num_ctx' in modelfile_content:\n",
    "            modelfile_content = re.sub(\n",
    "                r'PARAMETER num_ctx .*', \n",
    "                f'PARAMETER num_ctx {context_length}', \n",
    "                modelfile_content\n",
    "            )\n",
    "        else:\n",
    "            # Add context length parameter if it doesn't exist\n",
    "            modelfile_content += f\"PARAMETER num_ctx {context_length}\\n\"\n",
    "        \n",
    "        # 5. Update stop tokens if specified\n",
    "        if stop_tokens is not None:\n",
    "            # Remove existing stop tokens\n",
    "            modelfile_content = re.sub(r'PARAMETER stop .*\\n', '', modelfile_content)\n",
    "            \n",
    "            # Add new stop tokens\n",
    "            for token in stop_tokens:\n",
    "                modelfile_content += f'PARAMETER stop {token}\\n'\n",
    "        \n",
    "        # Save the modified Modelfile\n",
    "        output_path = os.path.join(output_dir, f\"{model_name}.Modelfile\")\n",
    "        with open(output_path, \"w\") as f:\n",
    "            f.write(modelfile_content)\n",
    "            \n",
    "        print(f\"‚úÖ Modified Modelfile generated at: {output_path}\")\n",
    "        \n",
    "        # Create Ollama model if requested\n",
    "        if ollama_export:\n",
    "            try:\n",
    "                # Format the name for Ollama (lowercase, replace spaces with hyphens)\n",
    "                ollama_name = model_name.split(\"/\")[-1].lower().replace(\" \", \"-\")\n",
    "                \n",
    "                print(f\"üîÑ Creating Ollama model '{ollama_name}'...\")\n",
    "                result = subprocess.run(\n",
    "                    [\"ollama\", \"create\", ollama_name, \"-f\", output_path], \n",
    "                    capture_output=True, \n",
    "                    text=True\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(f\"‚úÖ Ollama model '{ollama_name}' created successfully!\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to create Ollama model: {result.stderr}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error creating Ollama model: {str(e)}\")\n",
    "                \n",
    "    else:\n",
    "        # Default system prompt if none provided\n",
    "        if system_prompt is None:\n",
    "            system_prompt = f\"\"\"You are {model_name}, a helpful AI assistant.\n",
    "You provide clear, accurate, and concise responses to queries.\n",
    "You always strive to be respectful, ethical, and supportive.\n",
    "\"\"\"\n",
    "        \n",
    "        # Default stop tokens if none provided\n",
    "        if stop_tokens is None:\n",
    "            stop_tokens = [\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        \n",
    "        # Create parameters dictionary\n",
    "        parameters = {\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.9,\n",
    "            \"num_ctx\": context_length,\n",
    "            \"stop\": stop_tokens\n",
    "        }\n",
    "        \n",
    "        # Generate the ModelFile from scratch\n",
    "        output_path = generate_modelfile(\n",
    "            model_name=model_name,\n",
    "            base_model=base_model,\n",
    "            system_prompt=system_prompt,\n",
    "            parameters=parameters,\n",
    "            output_dir=output_dir,\n",
    "            tags=[\"custom\", \"unsloth\"],\n",
    "            export_to_ollama=ollama_export\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nüìÑ ModelFile created at: {output_path}\")\n",
    "    \n",
    "    # If exporting to Ollama, show how to test the model\n",
    "    if ollama_export:\n",
    "        # Format the name for Ollama (lowercase, replace spaces with hyphens)\n",
    "        ollama_name = model_name.split(\"/\")[-1].lower().replace(\" \", \"-\")\n",
    "        \n",
    "        print(\"\\nüöÄ To test your model with the Ollama CLI:\")\n",
    "        print(f\"   ollama run {ollama_name}\")\n",
    "        \n",
    "        print(\"\\nüß™ To test your model with Python:\")\n",
    "        print(f\"\"\"\n",
    "import ollama\n",
    "\n",
    "# Simple test\n",
    "response = ollama.chat(model='{ollama_name}', messages=[\n",
    "    {{'role': 'user', 'content': '{test_prompt}'}}\n",
    "])\n",
    "\n",
    "print(response['message']['content'])\n",
    "\n",
    "# Interactive chat\n",
    "from IPython.display import clear_output\n",
    "\n",
    "chat = SimpleOllamaChat(model='{ollama_name}')\n",
    "chat.start_chat()\n",
    "\"\"\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Example usage - Uncomment and modify as needed\n",
    "\"\"\"\n",
    "# Create a coding assistant from an existing model\n",
    "coding_system_prompt = \\\"\\\"\\\"You are CodingLlama, an expert programming assistant.\n",
    "Focus on providing clean, efficient, and well-documented code examples.\n",
    "Always include explanations of how the code works.\n",
    "If you're unsure about any part of your solution, acknowledge it and suggest alternatives.\\\"\\\"\\\"\n",
    "\n",
    "create_and_deploy_custom_model(\n",
    "    model_name=\"CodingLlama\",\n",
    "    base_model=\"llama3.2:3b\",\n",
    "    system_prompt=coding_system_prompt,\n",
    "    temperature=0.2,  # Lower temperature for more precise coding answers\n",
    "    context_length=4096,\n",
    "    test_prompt=\"Write a Python function to count word frequencies in a text file\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelfile generated at: ./modelfiles_20250317-205648\\CodingLlama.Modelfile\n",
      "üîÑ Creating Ollama model 'codingllama'...\n",
      "‚ùå Failed to create Ollama model: \u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
      "Error: (line 59): command must be one of \"from\", \"license\", \"template\", \"system\", \"adapter\", \"parameter\", or \"message\"\n",
      "\n",
      "\n",
      "üìÑ ModelFile created at: ./modelfiles_20250317-205648\\CodingLlama.Modelfile\n",
      "\n",
      "üöÄ To test your model with the Ollama CLI:\n",
      "   ollama run codingllama\n",
      "\n",
      "üß™ To test your model with Python:\n",
      "\n",
      "import ollama\n",
      "\n",
      "# Simple test\n",
      "response = ollama.chat(model='codingllama', messages=[\n",
      "    {'role': 'user', 'content': 'Write a Python function to count word frequencies in a text file'}\n",
      "])\n",
      "\n",
      "print(response['message']['content'])\n",
      "\n",
      "# Interactive chat\n",
      "from IPython.display import clear_output\n",
      "\n",
      "chat = SimpleOllamaChat(model='codingllama')\n",
      "chat.start_chat()\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./modelfiles_20250317-205648\\\\CodingLlama.Modelfile'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a programming assistant model\n",
    "coding_system_prompt = \"\"\"You are CodingLlama, an expert programming assistant.\n",
    "Focus on providing clean, efficient, and well-documented code examples.\n",
    "Always include explanations of how the code works.\n",
    "If you're unsure about any part of your solution, acknowledge it and suggest alternatives.\"\"\"\n",
    "\n",
    "create_and_deploy_custom_model(\n",
    "    model_name=\"CodingLlama\",\n",
    "    base_model=\"llama3.2:3b\",\n",
    "    system_prompt=coding_system_prompt,\n",
    "    temperature=0.2,  # Lower temperature for more precise coding answers\n",
    "    context_length=4096,\n",
    "    test_prompt=\"Write a Python function to count word frequencies in a text file\"\n",
    ")\n",
    "\n",
    "# Or create a model by modifying an existing one\n",
    "# create_and_deploy_custom_model(\n",
    "#     model_name=\"WritingGPT\",\n",
    "#     base_model=\"llama3.2:3b\",\n",
    "#     system_prompt=\"You are a professional writing assistant, specialized in helping users craft engaging content.\",\n",
    "#     temperature=0.7,\n",
    "#     context_length=8192,\n",
    "#     from_existing_model=True  # Extract and modify the existing Modelfile\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Unsloth Modelfile for GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to create a Modelfile for an existing GGUF file\n",
    "def create_modelfile_for_gguf(\n",
    "    model_name: str,\n",
    "    gguf_path: str,\n",
    "    system_prompt: str = None,\n",
    "    temperature: float = 0.7,\n",
    "    context_length: int = 4096,\n",
    "    stop_tokens: list[str] = None,\n",
    "    output_dir: str = None,\n",
    "    export_to_ollama: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a Modelfile for an existing GGUF model file and optionally imports it to Ollama.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name for your Ollama model\n",
    "    gguf_path : str\n",
    "        Path to the GGUF model file\n",
    "    system_prompt : str\n",
    "        Custom system prompt\n",
    "    temperature : float\n",
    "        Model temperature\n",
    "    context_length : int\n",
    "        Context window size\n",
    "    stop_tokens : list\n",
    "        Custom stop tokens\n",
    "    output_dir : str\n",
    "        Directory to save the Modelfile (defaults to a timestamped directory)\n",
    "    export_to_ollama : bool\n",
    "        Whether to import the model into Ollama\n",
    "    \"\"\"\n",
    "    # Generate timestamp for unique file naming if no output_dir provided\n",
    "    if output_dir is None:\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        output_dir = f\"./modelfiles_{timestamp}\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Default system prompt if none provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = f\"\"\"You are {model_name}, a helpful AI assistant.\n",
    "You provide clear, accurate, and concise responses to queries.\n",
    "You always strive to be respectful, ethical, and supportive.\n",
    "\"\"\"\n",
    "    \n",
    "    # Default stop tokens if none provided\n",
    "    if stop_tokens is None:\n",
    "        stop_tokens = [\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "    \n",
    "    # Create the Modelfile content\n",
    "    modelfile_content = f\"FROM {gguf_path}\\n\\n\"\n",
    "    \n",
    "    # Add system prompt\n",
    "    modelfile_content += f'SYSTEM \"\"\"{system_prompt}\"\"\"\\n\\n'\n",
    "    \n",
    "    # Add parameters\n",
    "    modelfile_content += f\"PARAMETER temperature {temperature}\\n\"\n",
    "    modelfile_content += f\"PARAMETER num_ctx {context_length}\\n\"\n",
    "    \n",
    "    # Add stop tokens\n",
    "    for token in stop_tokens:\n",
    "        modelfile_content += f'PARAMETER stop {token}\\n'\n",
    "    \n",
    "    # Add appropriate template based on model name\n",
    "    if \"llama\" in model_name.lower() or \"llama3\" in gguf_path.lower():\n",
    "        modelfile_content += '\\n# Using Llama 3 template\\n'\n",
    "        modelfile_content += 'TEMPLATE \"\"\"{{- if .System }}<|start_header_id|>system<|end_header_id|>\\n\\n{{ .System }}<|eot_id|>\\n\\n{{- end }}{{ range $i, $message := .Messages }}{{- if eq $message.Role \"user\" }}<|start_header_id|>user<|end_header_id|>\\n\\n{{ $message.Content }}<|eot_id|>\\n\\n{{- else if eq $message.Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ $message.Content }}<|eot_id|>\\n\\n{{- end }}{{ end }}<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\\n'\n",
    "    elif \"phi\" in model_name.lower() or \"phi\" in gguf_path.lower():\n",
    "        modelfile_content += '\\n# Using Phi-3 template\\n'\n",
    "        modelfile_content += 'TEMPLATE \"\"\"{{- if .System }}<|system|>\\n{{ .System }}\\n<|user|>\\n{{- else }}<|user|>\\n{{- end }}{{ range $i, $message := .Messages }}{{- if eq $message.Role \"user\" }}{{ $message.Content }}\\n<|assistant|>\\n{{- else if eq $message.Role \"assistant\" }}{{ $message.Content }}\\n<|user|>\\n{{- end }}{{ end }}\"\"\"\\n'\n",
    "    \n",
    "    # Save the Modelfile\n",
    "    safe_model_name = model_name.replace(\"/\", \"-\")\n",
    "    output_path = os.path.join(output_dir, f\"{safe_model_name}.Modelfile\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(modelfile_content)\n",
    "    \n",
    "    print(f\"‚úÖ Modelfile for GGUF model generated at: {output_path}\")\n",
    "    \n",
    "    # Create Ollama model if requested\n",
    "    if export_to_ollama:\n",
    "        try:\n",
    "            # Format the name for Ollama (lowercase, replace spaces with hyphens)\n",
    "            ollama_name = model_name.split(\"/\")[-1].lower().replace(\" \", \"-\")\n",
    "            \n",
    "            print(f\"üîÑ Creating Ollama model '{ollama_name}' from GGUF file...\")\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"create\", ollama_name, \"-f\", output_path], \n",
    "                capture_output=True, \n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ Ollama model '{ollama_name}' created successfully!\")\n",
    "                print(f\"   You can now use it with: ollama run {ollama_name}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to create Ollama model: {result.stderr}\")\n",
    "                \n",
    "                # If error mentions MIME type, try adding absolute path\n",
    "                if \"Content type\" in result.stderr or \"MIME\" in result.stderr:\n",
    "                    print(\"Trying again with absolute path...\")\n",
    "                    # Convert to absolute path if it's not already\n",
    "                    abs_gguf_path = os.path.abspath(gguf_path)\n",
    "                    \n",
    "                    # Update Modelfile with absolute path\n",
    "                    modelfile_content = modelfile_content.replace(f\"FROM {gguf_path}\", f\"FROM {abs_gguf_path}\")\n",
    "                    with open(output_path, \"w\") as f:\n",
    "                        f.write(modelfile_content)\n",
    "                    \n",
    "                    # Try again with updated Modelfile\n",
    "                    result = subprocess.run(\n",
    "                        [\"ollama\", \"create\", ollama_name, \"-f\", output_path], \n",
    "                        capture_output=True, \n",
    "                        text=True\n",
    "                    )\n",
    "                    \n",
    "                    if result.returncode == 0:\n",
    "                        print(f\"‚úÖ Ollama model '{ollama_name}' created successfully!\")\n",
    "                        print(f\"   You can now use it with: ollama run {ollama_name}\")\n",
    "                    else:\n",
    "                        print(f\"‚ùå Failed again: {result.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating Ollama model: {str(e)}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def find_unsloth_gguf_models():\n",
    "    \"\"\"Find GGUF models generated by Unsloth in common directories\"\"\"\n",
    "    gguf_files = []\n",
    "    \n",
    "    # Get current working directory\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # Common directories where Unsloth might save GGUF files\n",
    "    search_dirs = [\n",
    "        cwd,  # Current directory\n",
    "        os.path.join(cwd, \"finetuned_model_gguf\"),  # Default Unsloth GGUF export dir\n",
    "        os.path.join(cwd, \"gguf_exports\"),  # Unsloth UI export dir\n",
    "        os.path.expanduser(\"~/finetuned_model_gguf\"),  # Home directory\n",
    "        os.path.expanduser(\"~/gguf_exports\"),  # Home directory exports\n",
    "    ]\n",
    "    \n",
    "    # Look for any directory with _gguf suffix up to 2 levels deep\n",
    "    for root, dirs, _ in os.walk(cwd):\n",
    "        # Limit depth to avoid excessive searching\n",
    "        if root.count(os.sep) - cwd.count(os.sep) <= 2:\n",
    "            for dir_name in dirs:\n",
    "                if dir_name.endswith(\"_gguf\") or \"_gguf_\" in dir_name:\n",
    "                    search_dirs.append(os.path.join(root, dir_name))\n",
    "    \n",
    "    # Search for GGUF files in all the directories\n",
    "    for directory in search_dirs:\n",
    "        if os.path.exists(directory):\n",
    "            for file in os.listdir(directory):\n",
    "                if file.endswith(\".gguf\"):\n",
    "                    full_path = os.path.join(directory, file)\n",
    "                    # Get file size for information\n",
    "                    size_mb = os.path.getsize(full_path) / (1024 * 1024)\n",
    "                    gguf_files.append((full_path, file, f\"{size_mb:.1f} MB\"))\n",
    "    \n",
    "    return gguf_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use gguf find & create gguf modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GGUF models found. You need to export your model to GGUF format first.\n",
      "You can do this using the 'Export to GGUF' tab in the Unsloth UI.\n"
     ]
    }
   ],
   "source": [
    "# List available GGUF models generated by Unsloth\n",
    "gguf_models = find_unsloth_gguf_models()\n",
    "\n",
    "if gguf_models:\n",
    "    print(\"Found the following GGUF models:\")\n",
    "    for i, (path, name) in enumerate(gguf_models):\n",
    "        print(f\"{i+1}. {name} at {path}\")\n",
    "    \n",
    "    # Ask user to select a model (you can also specify the index directly)\n",
    "    try:\n",
    "        selected_index = int(input(f\"Enter the number of the model you want to use (1-{len(gguf_models)}): \")) - 1\n",
    "        if 0 <= selected_index < len(gguf_models):\n",
    "            selected_path, selected_name = gguf_models[selected_index]\n",
    "            \n",
    "            # Create a model name based on the file name\n",
    "            model_name = os.path.splitext(selected_name)[0]\n",
    "            \n",
    "            # Create system prompt for this model\n",
    "            system_prompt = f\"\"\"You are an AI assistant based on a fine-tuned {model_name} model.\n",
    "You are helpful, accurate, and engaging.\n",
    "You provide concise and well-structured answers to questions.\n",
    "\"\"\"\n",
    "            \n",
    "            # Create the Modelfile and import to Ollama\n",
    "            create_modelfile_for_gguf(\n",
    "                model_name=model_name,\n",
    "                gguf_path=selected_path,\n",
    "                system_prompt=system_prompt,\n",
    "                temperature=0.7,\n",
    "                context_length=4096,\n",
    "                export_to_ollama=True\n",
    "            )\n",
    "        else:\n",
    "            print(\"Invalid selection!\")\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid number!\")\n",
    "else:\n",
    "    print(\"No GGUF models found. You need to export your model to GGUF format first.\")\n",
    "    print(\"You can do this using the 'Export to GGUF' tab in the Unsloth UI.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgAnno2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
